{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10793343,"sourceType":"datasetVersion","datasetId":6698197}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\n# Load dataset and create a copy\ndf_original = pd.read_csv(\"/kaggle/input/rt-iot2022/RT_IOT2022.csv\")\ndf = df_original.copy()\n\nprint(\"Original dataset shape:\", df_original.shape)\nprint(\"Original Attack Types:\", df_original['Attack_type'].unique())\n\n# Check for missing values\nprint(\"\\nMissing values in dataset:\", df.isnull().sum().sum())\n\n# Identify categorical columns\ncategorical_columns = df.select_dtypes(include=['object']).columns\nprint(\"\\nCategorical Columns:\", categorical_columns)\n\n# Make a copy of Attack_type before encoding\nattack_type_copy = df['Attack_type'].copy()\n\n# Encode categorical columns\nlabel_encoders = {}\nfor col in categorical_columns:\n    if col != 'Attack_type':  # Skip Attack_type for now\n        label_encoders[col] = LabelEncoder()\n        df[col] = label_encoders[col].fit_transform(df[col])\n\n# Now encode Attack_type separately\nlabel_encoders['Attack_type'] = LabelEncoder()\ndf['Attack_type'] = label_encoders['Attack_type'].fit_transform(attack_type_copy)\n\n# Store the mapping of Attack_type labels\nattack_type_mapping = dict(zip(attack_type_copy, df['Attack_type']))\nprint(\"\\nAttack Type Mapping:\")\nprint(attack_type_mapping)\n\n# Create binary attack labels\nif 'Attack_type' in df.columns:\n    print(\"\\nUnique Attack Types:\", df['Attack_type'].unique())\n\n    # Check if 'Normal' exists before transforming\n    if 'Normal' in label_encoders['Attack_type'].classes_:\n        normal_label = label_encoders['Attack_type'].transform(['Normal'])[0]\n        df['Attack_label'] = (df['Attack_type'] != normal_label).astype(int)\n    else:\n        print(\"⚠️ Warning: 'Normal' label is missing from the dataset. Assigning default label.\")\n        df['Attack_label'] = (df['Attack_type'] != df['Attack_type'].mode()[0]).astype(int)\n\n    print(\"\\nBinary Label Distribution:\")\n    print(df['Attack_label'].value_counts())\nelse:\n    print(\"⚠️ 'Attack_type' column is missing. Check dataset format.\")\n\n# Check for infinite values\nprint(\"\\nInfinite values in dataset:\", np.isinf(df.select_dtypes(include=[np.number])).sum().sum())\n\n# Function to clean dataset\ndef clean_dataset(df):\n    df_clean = df.copy()\n    \n    # Replace infinite values with NaN\n    df_clean.replace([np.inf, -np.inf], np.nan, inplace=True)\n    \n    # Drop columns with too many missing values (>50%)\n    threshold = len(df_clean) * 0.5\n    df_clean.dropna(thresh=threshold, axis=1, inplace=True)\n    \n    # Fill remaining NaN with median values\n    df_clean.fillna(df_clean.median(), inplace=True)\n    \n    return df_clean\n\n# Clean the dataset\ndf_processed = clean_dataset(df)\n\n# Final check for NaN values\nprint(\"Final check for missing values:\", df_processed.isnull().sum().sum())\n\n# Check for class imbalance\nif 'Attack_label' in df_processed.columns and df_processed['Attack_label'].nunique() == 1:\n    majority_class = df_processed['Attack_label'].iloc[0]\n    print(f\"\\n⚠️ WARNING: Only one class ({majority_class}) found in the dataset!\")\n\n    if majority_class == 1:  # All attacks\n        print(\"Attempting to find original dataset with normal samples...\")\n        try:\n            additional_df = pd.read_csv(\"/kaggle/input/rt-iot2022/Original_RT_IOT2022.csv\")\n            \n            if 'Attack_type' in additional_df.columns:\n                additional_df['Attack_label'] = (additional_df['Attack_type'] != 'Normal').astype(int)\n                \n                if additional_df['Attack_label'].nunique() > 1:\n                    print(\"✅ Original dataset contains both classes. Using it instead.\")\n                    normal_samples = additional_df[additional_df['Attack_label'] == 0].sample(\n                        min(1000, len(additional_df[additional_df['Attack_label'] == 0])),\n                        random_state=42\n                    )\n                    attack_samples = df_processed.sample(min(1000, len(df_processed)), random_state=42)\n                    df_processed = pd.concat([normal_samples, attack_samples]).reset_index(drop=True)\n                    print(f\"New dataset shape: {df_processed.shape}\")\n                    print(f\"New class distribution: {df_processed['Attack_label'].value_counts()}\")\n        except:\n            print(\"Could not find original dataset with normal samples.\")\n            print(\"Creating synthetic normal samples...\")\n\n            attack_samples = df_processed.sample(100, random_state=42)\n            normal_samples = attack_samples.copy()\n            numeric_cols = normal_samples.select_dtypes(include=[np.number]).columns\n            \n            for col in numeric_cols:\n                if col != 'Attack_label':\n                    std = normal_samples[col].std()\n                    normal_samples[col] = normal_samples[col] + np.random.normal(0, std/2, len(normal_samples))\n\n            if 'Attack_type' in normal_samples.columns:\n                normal_samples['Attack_type'] = 'Normal'\n            normal_samples['Attack_label'] = 0\n            \n            df_processed = pd.concat([df_processed, normal_samples]).reset_index(drop=True)\n            \n            print(f\"New dataset with synthetic samples shape: {df_processed.shape}\")\n            print(f\"New class distribution: {df_processed['Attack_label'].value_counts()}\")\n\n# Separate features and target\nX = df_processed.drop(columns=[\"Attack_type\", \"Attack_label\"])\ny = df_processed[\"Attack_label\"]  # Using binary labels for classification\n\n# Normalize numerical features\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Split into training and test sets (80-20 split)\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n\nprint(\"\\nDataset preprocessed successfully!\")\nprint(\"Original dataset shape:\", df_original.shape)\nprint(\"Processed dataset shape:\", df_processed.shape)\nprint(\"Training data shape:\", X_train.shape)\nprint(\"Testing data shape:\", X_test.shape)\n\n# Save the mappings and processed data if needed\nlabel_encoder_mappings = {\n    col: dict(zip(label_encoders[col].classes_, label_encoders[col].transform(label_encoders[col].classes_)))\n    for col in label_encoders\n}\nprint(\"\\nLabel Encoder Mappings saved for future reference\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.metrics import accuracy_score, classification_report, roc_auc_score, confusion_matrix\nfrom imblearn.over_sampling import SMOTE\nimport joblib\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n# Step 1: Preprocess Data\ndf = df.dropna()  # Remove NaN values\nX = df.drop(columns=['Attack_label'])\ny = df['Attack_label']\n# Encode categorical variables\nfor col in X.select_dtypes(include=['object']).columns:\n    X[col] = LabelEncoder().fit_transform(X[col])\n# Standardize numerical features\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n# Balance the dataset using SMOTE\nsmote = SMOTE(random_state=42)\nX_resampled, y_resampled = smote.fit_resample(X_scaled, y)\n# Train-Test Split\nX_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled)\n# Step 2: Build a More Robust Autoencoder Model\ninput_dim = X_train.shape[1]\nautoencoder = keras.models.Sequential([\n    keras.layers.Dense(128, activation=\"relu\", input_shape=(input_dim,)),\n    keras.layers.BatchNormalization(),\n    keras.layers.Dropout(0.3),\n    keras.layers.Dense(64, activation=\"relu\"),\n    keras.layers.BatchNormalization(),\n    keras.layers.Dropout(0.3),\n    keras.layers.Dense(32, activation=\"relu\"),\n    keras.layers.BatchNormalization(),\n    keras.layers.Dense(64, activation=\"relu\"),\n    keras.layers.Dense(128, activation=\"relu\"),\n    keras.layers.Dense(input_dim, activation=\"linear\")  # Reconstruct original input\n])\nautoencoder.compile(optimizer=\"adam\", loss=\"mse\")\n# Step 3: Train Autoencoder on Normal Data\nX_train_normal = X_train[y_train == 0]  # Only use normal samples for training\nhistory = autoencoder.fit(\n    X_train_normal, X_train_normal, \n    epochs=200, batch_size=64, \n    validation_split=0.1, verbose=1\n)\n# Step 4: Anomaly Detection using Reconstruction Error\nreconstructed = autoencoder.predict(X_test)\nmse = np.mean(np.square(X_test - reconstructed), axis=1)\n# Step 5: Dynamic Threshold Calculation\nthreshold = np.percentile(mse, 95)  # 95th percentile of normal reconstruction errors\ny_pred_autoencoder = (mse > threshold).astype(int)  # 1 for anomaly, 0 for normal\n# Step 6: Isolation Forest with Hyperparameter Tuning\niso_forest = IsolationForest(n_estimators=500, contamination=\"auto\", random_state=42)\niso_forest.fit(mse.reshape(-1, 1))\ny_pred_iso = iso_forest.predict(mse.reshape(-1, 1))\ny_pred_iso = np.where(y_pred_iso == -1, 1, 0)  # Convert -1 (anomaly) to 1 (attack)\n# Step 7: Combine Both Models (Majority Voting)\ny_final_pred = (y_pred_autoencoder + y_pred_iso) > 0  # If either model predicts anomaly, classify as attack\ny_final_pred = y_final_pred.astype(int)\n# Step 8: Model Evaluation\naccuracy = accuracy_score(y_test, y_final_pred)\nroc_auc = roc_auc_score(y_test, mse)\nconf_matrix = confusion_matrix(y_test, y_final_pred)\nprint(\"\\n🔹 Accuracy:\", accuracy)\nprint(\"\\n🔹 ROC-AUC Score:\", roc_auc)\nprint(\"\\n🔹 Classification Report (Autoencoder + Isolation Forest):\\n\", classification_report(y_test, y_final_pred))\n# Step 9: Confusion Matrix Visualization\nplt.figure(figsize=(6,4))\nsns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Normal\", \"Attack\"], yticklabels=[\"Normal\", \"Attack\"])\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.title(\"Confusion Matrix\")\nplt.show()\n# Step 10: Save the Models\nautoencoder.save(\"autoencoder_model.h5\")\njoblib.dump(scaler, \"scaler.pkl\")\njoblib.dump(iso_forest, \"isolation_forest.pkl\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow.keras.backend as K\nimport numpy as np\n\n# Step 1: Recompute mse using the teacher model on training data and compute teacher soft labels\nreconstructed_train = autoencoder.predict(X_train)\nmse_train = np.mean(np.square(X_train - reconstructed_train), axis=1)\nmse_scaled = (mse_train - np.min(mse_train)) / (np.max(mse_train) - np.min(mse_train))\nteacher_preds_np = 1 / (1 + np.exp(-10 * (mse_scaled - 0.5)))  # Sigmoid transformation\nteacher_preds_np = np.expand_dims(teacher_preds_np, axis=-1)  # shape: (n_samples, 1)\n\n# Convert teacher predictions to a TensorFlow tensor (we'll use them per sample)\nteacher_preds_tensor = tf.convert_to_tensor(teacher_preds_np, dtype=tf.float32)\n\n# Step 2: Build the tf.data.Dataset that yields (x, y, teacher_pred)\nbatch_size = 64\ndataset = tf.data.Dataset.from_tensor_slices((X_train, y_train, teacher_preds_tensor))\ndataset = dataset.shuffle(buffer_size=1024).batch(batch_size)\n\n# Step 3: Define the Student Model (same as before)\nstudent_model = keras.models.Sequential([\n    keras.layers.Dense(64, activation=\"relu\", input_shape=(input_dim,)),\n    keras.layers.BatchNormalization(),\n    keras.layers.Dropout(0.2),\n    keras.layers.Dense(32, activation=\"relu\"),\n    keras.layers.BatchNormalization(),\n    keras.layers.Dense(1, activation=\"sigmoid\")  # Binary classification\n])\n\noptimizer = tf.keras.optimizers.Adam()\n\n# Step 4: Define the distillation loss as a function that works on a batch\ndef distillation_loss(y_true, y_pred, teacher_preds, temperature=5.0, alpha=0.7):\n    # Hard loss: Binary crossentropy\n    hard_loss = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n    # Soft loss: KL divergence between softened teacher and student outputs\n    teacher_probs = tf.nn.softmax(teacher_preds / temperature)\n    student_probs = tf.nn.softmax(y_pred / temperature)\n    soft_loss = tf.keras.losses.KLDivergence()(teacher_probs, student_probs)\n    # Combine the losses\n    return alpha * hard_loss + (1 - alpha) * soft_loss\n\n# Step 5: Custom training loop\nepochs = 100\n\nfor epoch in range(epochs):\n    epoch_loss = 0.0\n    for step, (x_batch, y_batch, teacher_batch) in enumerate(dataset):\n        with tf.GradientTape() as tape:\n            # Forward pass\n            predictions = student_model(x_batch, training=True)\n            # Compute loss using the current batch's teacher predictions\n            loss = distillation_loss(tf.cast(tf.expand_dims(y_batch, axis=-1), tf.float32),\n                                     predictions,\n                                     teacher_batch)\n        # Compute gradients and update weights\n        gradients = tape.gradient(loss, student_model.trainable_variables)\n        optimizer.apply_gradients(zip(gradients, student_model.trainable_variables))\n        epoch_loss += tf.reduce_mean(loss)\n    print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss.numpy()/ (step+1)}\")\n\n# Step 6: Evaluate the student model on X_test\nstudent_preds = student_model.predict(X_test)\nstudent_preds_binary = (student_preds > 0.5).astype(int)\n\naccuracy_student = accuracy_score(y_test, student_preds_binary)\nroc_auc_student = roc_auc_score(y_test, student_preds_binary)\n\nprint(\"\\n🔹 Student Model Accuracy:\", accuracy_student)\nprint(\"\\n🔹 Student Model ROC-AUC Score:\", roc_auc_student)\nprint(\"\\n🔹 Classification Report (Student Model):\\n\", classification_report(y_test, student_preds_binary))\n\n# Optionally, save the student model\nstudent_model.save(\"student_model.h5\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}