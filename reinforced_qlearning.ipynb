{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded and preprocessed successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"RT_IOT2022.csv\"  # Update this if the file is in a different location\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "df = df.drop(columns=[\"Unnamed: 0\"], errors='ignore')\n",
    "# Encode categorical features\n",
    "label_encoder = LabelEncoder()\n",
    "df[\"proto\"] = label_encoder.fit_transform(df[\"proto\"])\n",
    "df[\"service\"] = label_encoder.fit_transform(df[\"service\"])\n",
    "df[\"Attack_type\"] = label_encoder.fit_transform(df[\"Attack_type\"])  # Target variable\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop(columns=[\"Attack_type\"])\n",
    "y = df[\"Attack_type\"]\n",
    "\n",
    "# Normalize numerical features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split into training and test sets (80-20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Dataset loaded and preprocessed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training teacher Q-learning model...\n",
      "Episode 0/1000, Epsilon: 0.9950\n",
      "Episode 100/1000, Epsilon: 0.6027\n",
      "Episode 200/1000, Epsilon: 0.3651\n",
      "Episode 300/1000, Epsilon: 0.2212\n",
      "Episode 400/1000, Epsilon: 0.1340\n",
      "Episode 500/1000, Epsilon: 0.0812\n",
      "Episode 600/1000, Epsilon: 0.0492\n",
      "Episode 700/1000, Epsilon: 0.0298\n",
      "Episode 800/1000, Epsilon: 0.0180\n",
      "Episode 900/1000, Epsilon: 0.0109\n",
      "Teacher model training complete!\n",
      "Performing knowledge distillation to student Q-learning model...\n",
      "Distillation Episode 0/500, Epsilon: 0.4950, Loss: 5451.1331\n",
      "Distillation Episode 50/500, Epsilon: 0.2995, Loss: 231.6604\n",
      "Distillation Episode 100/500, Epsilon: 0.1812, Loss: 74.0678\n",
      "Distillation Episode 150/500, Epsilon: 0.1096, Loss: 31.2263\n",
      "Distillation Episode 200/500, Epsilon: 0.0663, Loss: 16.5936\n",
      "Distillation Episode 250/500, Epsilon: 0.0500, Loss: 11.0309\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from imblearn.combine import SMOTETomek\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix\n",
    "\n",
    "# Apply Hybrid Sampling (SMOTE + Tomek Links)\n",
    "smote_tomek = SMOTETomek()\n",
    "X_train_bal, y_train_bal = smote_tomek.fit_resample(X_train, y_train)\n",
    "\n",
    "# Normalize Features\n",
    "scaler = StandardScaler()\n",
    "X_train_bal = scaler.fit_transform(X_train_bal)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Hyperparameters for teacher model\n",
    "teacher_params = {\n",
    "    \"alpha\": 0.1,  # Learning rate\n",
    "    \"gamma\": 0.9,  # Discount factor\n",
    "    \"epsilon\": 1.0,  # Exploration rate\n",
    "    \"epsilon_decay\": 0.995,\n",
    "    \"epsilon_min\": 0.01,\n",
    "    \"num_episodes\": 1000,\n",
    "    \"bins\": 10  # For state discretization\n",
    "}\n",
    "\n",
    "# Hyperparameters for student model (simpler/faster model)\n",
    "student_params = {\n",
    "    \"alpha\": 0.2,  # Higher learning rate for faster convergence\n",
    "    \"gamma\": 0.8,  # Slightly lower discount factor for simpler behavior\n",
    "    \"epsilon\": 0.5,  # Lower initial exploration (leveraging teacher knowledge)\n",
    "    \"epsilon_decay\": 0.99,\n",
    "    \"epsilon_min\": 0.05,\n",
    "    \"num_episodes\": 500,  # Fewer episodes needed due to knowledge transfer\n",
    "    \"bins\": 5  # Coarser state discretization for faster lookups\n",
    "}\n",
    "\n",
    "# Get number of actions from the training data\n",
    "num_actions = len(np.unique(y_train_bal))\n",
    "\n",
    "# Initialize Q-tables for teacher and student\n",
    "Q_teacher = defaultdict(lambda: np.zeros(num_actions))\n",
    "Q_student = defaultdict(lambda: np.zeros(num_actions))\n",
    "\n",
    "# Function to discretize states for teacher\n",
    "def discretize_state_teacher(state):\n",
    "    \"\"\"Convert continuous state to discrete state for teacher Q-table lookup\"\"\"\n",
    "    discretized = tuple(np.floor(state * teacher_params[\"bins\"]).astype(int))\n",
    "    return discretized\n",
    "\n",
    "# Function to discretize states for student (coarser discretization)\n",
    "def discretize_state_student(state):\n",
    "    \"\"\"Convert continuous state to discrete state for student Q-table lookup\"\"\"\n",
    "    discretized = tuple(np.floor(state * student_params[\"bins\"]).astype(int))\n",
    "    return discretized\n",
    "\n",
    "# Epsilon-greedy action selection for teacher\n",
    "def choose_action_teacher(state, epsilon):\n",
    "    discretized_state = discretize_state_teacher(state)\n",
    "    \n",
    "    if np.random.rand() < epsilon:\n",
    "        return random.choice(range(num_actions))\n",
    "    else:\n",
    "        return np.argmax(Q_teacher[discretized_state])\n",
    "\n",
    "# Epsilon-greedy action selection for student\n",
    "def choose_action_student(state, epsilon):\n",
    "    discretized_state = discretize_state_student(state)\n",
    "    \n",
    "    if np.random.rand() < epsilon:\n",
    "        return random.choice(range(num_actions))\n",
    "    else:\n",
    "        return np.argmax(Q_student[discretized_state])\n",
    "\n",
    "# Training function for the teacher Q-learning model\n",
    "def train_teacher_model():\n",
    "    print(\"Training teacher Q-learning model...\")\n",
    "    epsilon = teacher_params[\"epsilon\"]\n",
    "    epsilon_history = []\n",
    "    \n",
    "    for episode in range(teacher_params[\"num_episodes\"]):\n",
    "        # Shuffle data at the beginning of each episode\n",
    "        indices = np.arange(len(X_train_bal))\n",
    "        np.random.shuffle(indices)\n",
    "        X_shuffled = X_train_bal[indices]\n",
    "        y_shuffled = y_train_bal[indices]\n",
    "        \n",
    "        for i in range(len(X_shuffled)):\n",
    "            state = X_shuffled[i]\n",
    "            discretized_state = discretize_state_teacher(state)\n",
    "            action = choose_action_teacher(state, epsilon)\n",
    "            \n",
    "            # Reward Function\n",
    "            if action == y_shuffled[i]:\n",
    "                reward = 10 if action != 0 else 5\n",
    "            else:\n",
    "                reward = -10 if action != 0 else -5\n",
    "            \n",
    "            # Get next state\n",
    "            next_state = X_shuffled[(i + 1) % len(X_shuffled)]\n",
    "            discretized_next_state = discretize_state_teacher(next_state)\n",
    "            \n",
    "            # Q-learning update\n",
    "            best_next_action = np.argmax(Q_teacher[discretized_next_state])\n",
    "            td_target = reward + teacher_params[\"gamma\"] * Q_teacher[discretized_next_state][best_next_action]\n",
    "            td_error = td_target - Q_teacher[discretized_state][action]\n",
    "            Q_teacher[discretized_state][action] += teacher_params[\"alpha\"] * td_error\n",
    "        \n",
    "        # Store epsilon for plotting\n",
    "        epsilon_history.append(epsilon)\n",
    "        \n",
    "        # Decay epsilon\n",
    "        epsilon = max(teacher_params[\"epsilon_min\"], epsilon * teacher_params[\"epsilon_decay\"])\n",
    "        \n",
    "        if episode % 100 == 0:\n",
    "            print(f\"Episode {episode}/{teacher_params['num_episodes']}, Epsilon: {epsilon:.4f}\")\n",
    "    \n",
    "    print(\"Teacher model training complete!\")\n",
    "    return epsilon_history\n",
    "\n",
    "# Knowledge distillation function to transfer knowledge from teacher to student\n",
    "def distill_knowledge_to_student():\n",
    "    print(\"Performing knowledge distillation to student Q-learning model...\")\n",
    "    epsilon = student_params[\"epsilon\"]\n",
    "    distillation_history = []\n",
    "    \n",
    "    # Temperature parameter for softening Q-values\n",
    "    temperature = 2.0\n",
    "    \n",
    "    for episode in range(student_params[\"num_episodes\"]):\n",
    "        episode_loss = 0\n",
    "        \n",
    "        # Shuffle data at the beginning of each episode\n",
    "        indices = np.arange(len(X_train_bal))\n",
    "        np.random.shuffle(indices)\n",
    "        X_shuffled = X_train_bal[indices]\n",
    "        \n",
    "        for i in range(len(X_shuffled)):\n",
    "            state = X_shuffled[i]\n",
    "            teacher_state = discretize_state_teacher(state)\n",
    "            student_state = discretize_state_student(state)\n",
    "            \n",
    "            # Get teacher's Q-values (soft targets)\n",
    "            teacher_q_values = Q_teacher[teacher_state]\n",
    "            \n",
    "            # Convert to probabilities via softmax with temperature\n",
    "            teacher_q = teacher_q_values / temperature\n",
    "            exp_teacher_q = np.exp(teacher_q - np.max(teacher_q))  # For numerical stability\n",
    "            teacher_probs = exp_teacher_q / np.sum(exp_teacher_q)\n",
    "            \n",
    "            # Student selects action (with some exploration)\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = random.choice(range(num_actions))\n",
    "            else:\n",
    "                # Sometimes follow teacher's advice, sometimes use own policy\n",
    "                if np.random.rand() < 0.7:  # 70% follow teacher\n",
    "                    action = np.argmax(teacher_q_values)\n",
    "                else:\n",
    "                    action = np.argmax(Q_student[student_state])\n",
    "            \n",
    "            # Update student's Q-values to mimic teacher's Q-values\n",
    "            for a in range(num_actions):\n",
    "                # Calculate loss (MSE between teacher and student Q-values)\n",
    "                target_q = teacher_q_values[a]\n",
    "                current_q = Q_student[student_state][a]\n",
    "                loss = (target_q - current_q) ** 2\n",
    "                episode_loss += loss\n",
    "                \n",
    "                # Update student's Q-value toward teacher's Q-value\n",
    "                if a == action:\n",
    "                    # Stronger update for the selected action\n",
    "                    Q_student[student_state][a] += student_params[\"alpha\"] * (target_q - current_q)\n",
    "                else:\n",
    "                    # Weaker updates for non-selected actions (optional)\n",
    "                    Q_student[student_state][a] += 0.01 * student_params[\"alpha\"] * (target_q - current_q)\n",
    "        \n",
    "        # Store average loss for plotting\n",
    "        distillation_history.append(episode_loss / len(X_shuffled))\n",
    "        \n",
    "        # Decay epsilon\n",
    "        epsilon = max(student_params[\"epsilon_min\"], epsilon * student_params[\"epsilon_decay\"])\n",
    "        \n",
    "        if episode % 50 == 0:\n",
    "            print(f\"Distillation Episode {episode}/{student_params['num_episodes']}, Epsilon: {epsilon:.4f}, Loss: {distillation_history[-1]:.4f}\")\n",
    "    \n",
    "    print(\"Knowledge distillation complete!\")\n",
    "    return distillation_history\n",
    "\n",
    "# Evaluation function for Q-learning models\n",
    "def evaluate_q_model(Q_table, discretize_fn, X_data, y_data, model_name):\n",
    "    y_pred = []\n",
    "    for state in X_data:\n",
    "        discretized_state = discretize_fn(state)\n",
    "        action = np.argmax(Q_table[discretized_state])\n",
    "        y_pred.append(action)\n",
    "    \n",
    "    y_pred = np.array(y_pred)\n",
    "    accuracy = accuracy_score(y_data, y_pred)\n",
    "    precision = precision_score(y_data, y_pred, average=\"weighted\")\n",
    "    recall = recall_score(y_data, y_pred, average=\"weighted\")\n",
    "    f1 = f1_score(y_data, y_pred, average=\"weighted\")\n",
    "    cm = confusion_matrix(y_data, y_pred)\n",
    "    \n",
    "    print(f\"\\n{model_name} Evaluation:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        \"Model\": model_name,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1 Score\": f1,\n",
    "        \"Confusion Matrix\": cm\n",
    "    }\n",
    "\n",
    "# Train teacher model\n",
    "teacher_epsilon_history = train_teacher_model()\n",
    "\n",
    "# Perform knowledge distillation\n",
    "distillation_loss_history = distill_knowledge_to_student()\n",
    "\n",
    "# Evaluate both models on test data\n",
    "teacher_metrics = evaluate_q_model(Q_teacher, discretize_state_teacher, X_test_scaled, y_test, \"Teacher Q-Learning\")\n",
    "student_metrics = evaluate_q_model(Q_student, discretize_state_student, X_test_scaled, y_test, \"Student Q-Learning\")\n",
    "\n",
    "# Display comparative results in a DataFrame\n",
    "results = [\n",
    "    teacher_metrics,\n",
    "    student_metrics\n",
    "]\n",
    "results_df = pd.DataFrame([\n",
    "    {k: v for k, v in result.items() if k != \"Confusion Matrix\"} \n",
    "    for result in results\n",
    "])\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(results_df)\n",
    "\n",
    "# Analyze model size and efficiency\n",
    "teacher_size = len(Q_teacher)\n",
    "student_size = len(Q_student)\n",
    "size_reduction = (1 - student_size / teacher_size) * 100 if teacher_size > 0 else 0\n",
    "\n",
    "print(f\"\\nModel Size Comparison:\")\n",
    "print(f\"Teacher Q-table entries: {teacher_size}\")\n",
    "print(f\"Student Q-table entries: {student_size}\")\n",
    "print(f\"Size reduction: {size_reduction:.2f}%\")\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Plot epsilon decay during teacher training\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(teacher_epsilon_history)\n",
    "plt.title(\"Teacher Model: Epsilon Decay\")\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Epsilon\")\n",
    "\n",
    "# Plot loss during knowledge distillation\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(distillation_loss_history)\n",
    "plt.title(\"Knowledge Distillation: Loss Over Episodes\")\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Average Loss\")\n",
    "\n",
    "# Plot performance comparison\n",
    "plt.subplot(2, 2, 3)\n",
    "metrics = [\"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\"]\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "teacher_values = [teacher_metrics[m] for m in metrics]\n",
    "student_values = [student_metrics[m] for m in metrics]\n",
    "\n",
    "plt.bar(x - width/2, teacher_values, width, label='Teacher Q-Learning')\n",
    "plt.bar(x + width/2, student_values, width, label='Student Q-Learning')\n",
    "plt.xlabel('Metrics')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Performance Comparison')\n",
    "plt.xticks(x, metrics)\n",
    "plt.legend()\n",
    "\n",
    "# Plot comparison of Q-value distributions\n",
    "plt.subplot(2, 2, 4)\n",
    "\n",
    "# Extract Q-values from both models\n",
    "teacher_q_values = np.array([np.max(Q_teacher[state]) for state in list(Q_teacher.keys())[:100]])\n",
    "student_q_values = np.array([np.max(Q_student[state]) for state in list(Q_student.keys())[:100]])\n",
    "\n",
    "plt.hist(teacher_q_values, alpha=0.5, label='Teacher Q-values')\n",
    "plt.hist(student_q_values, alpha=0.5, label='Student Q-values')\n",
    "plt.title('Q-value Distributions (Sample)')\n",
    "plt.xlabel('Q-value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot confusion matrices\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(teacher_metrics[\"Confusion Matrix\"], cmap='Blues')\n",
    "plt.title('Teacher Q-Learning Confusion Matrix')\n",
    "plt.colorbar()\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(student_metrics[\"Confusion Matrix\"], cmap='Blues')\n",
    "plt.title('Student Q-Learning Confusion Matrix')\n",
    "plt.colorbar()\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Machines",
   "language": "python",
   "name": "machines"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
